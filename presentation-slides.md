---
marp: true
theme: default
paginate: true
header: 'Kafka: From Zero to Semi-Hero'
footer: 'Connected Train Platform Workshop | 2025'
style: |
  /* Base slide styling and sizing to reduce clipping on dense slides */
  section {
    background-color: #ffffff;
    color: #333333;
    font-size: 20px; /* base font size, smaller so more content fits */
    line-height: 1.15;
    padding: 2rem;
    box-sizing: border-box;
    overflow-y: auto; /* allow scrolling in HTML output to avoid cut content */
  }
  /* Use these helpers on particularly dense slides: add <!-- _class: compact --> */
  .compact {
    font-size: 16px;
    line-height: 1.08;
  }
  .small {
    font-size: 14px;
    line-height: 1.05;
  }
  h1 {
    color: #231F20;
    font-size: 2.2em;
  }
  h2 {
    color: #231F20;
  }
  code {
    background-color: #f4f4f4;
    padding: 0.12rem 0.25rem;
    border-radius: 4px;
  }
  ul, ol {
    margin-top: 0.5rem;
    margin-bottom: 0.5rem;
  }
  p { margin: 0.25rem 0; }
  .columns {
    display: grid;
    grid-template-columns: repeat(2, minmax(0, 1fr));
    gap: 1rem;
  }
  .about {
    border-radius: 12px;
    padding: 1rem 1.25rem;
    background: linear-gradient(135deg, #f0f7ff 0%, #ffffff 100%);
    border: 1px solid rgba(35,31,32,0.06);
    box-shadow: 0 6px 18px rgba(35,31,32,0.06);
    max-width: 900px;
    margin: 1rem 0 0 0;
  }
  .person {
    margin-bottom: 0.6rem;
  }
  .person h4 {
    margin: 0 0 0.15rem 0;
    font-size: 1.05rem;
  }
  .person p {
    margin: 0;
    font-size: 0.92rem;
    color: #333;
  }
---

<!-- _class: lead -->
<!-- _paginate: false -->

# Kafka: From Zero to Semi-Hero ğŸš€

## Building Event-Driven Systems with Apache Kafka

### Connected Train Platform Workshop ğŸš‚

---
  ### About Us
<section class="about">


  <div class="person">
  <h4>Francesco Lacriola â€” Software Engineer, Senior Analyst</h4>
  <p>I'm Francesco Lacriola, a Senior Analyst & Software Engineer at Fincons Group, where I have worked for over six years. I've contributed to large-scale projects across Energy, Finance and Transportation (currently on the SBB project). My role blends technical expertise, curiosity for innovation and a strong focus on software quality. I enjoy exploring cutting-edge technologies and improving development practices. I believe in teamwork, continuous improvement and knowledge sharing as drivers for personal and professional growth. Outside of technology, my greatest passion is music â€” it helps me relax and recharge. ğŸ§</p>
  </div>

  <div class="person">
  <h4>Michele Caccia â€” Senior Consultant</h4>
  <p>I'm Michele Caccia, a Senior Consultant at Fincons since 2019. After a short but formative experience at Corner Bank, I joined the SBB project in Bern. I've been a technology enthusiast since childhood and I enjoy exploring new tools and approaches that help in everyday engineering. I'm strongly motivated and thrive in team-based environments.</p>
  </div>

  <div class="person">
  <h4>Daniele Gallitelli â€” Software Engineer, Senior Analyst</h4>
  <p>I'm Daniele Gallitelli, a Senior Analyst & Software Engineer with almost six years at Fincons Group. I work on the SBB project as a Java backend (and sometimes full-stack) developer. I'm a Scrum enthusiast and I focus on practical, solution-oriented approaches. In short: like Mr. Wolf â€” I solve problems!</p>
  </div>

</section>


<!-- _class: compact -->
---
# What We'll Build Today ğŸ¯

- A real-world, multi-service event-driven system
- Simulates a fleet of trains sending real-time location data
- Processes data to generate insights
- Visualizes everything on a live dashboard
- Uses multiple programming languages (Java & Python)

**Architecture:** Train â†’ Kafka â†’ Services â†’ Dashboard

---

# What is Apache Kafka? ğŸ“š
<!-- _class: compact -->

**Distributed streaming platform** for building real-time data pipelines

- Originally developed by LinkedIn, now open-source (Apache)
- Think of it as a "nervous system" for data in your organization
- Handles **trillions** of messages per day at companies like Netflix, Uber, LinkedIn

### Key Features:
âœ… High throughput & low latency
âœ… Fault-tolerant & scalable
âœ… Persistent storage
âœ… Real-time processing

---

<!-- _class: compact -->
# Why Kafka? ğŸ¤”

- ğŸš€ High throughput & low latency â€” built to handle millions of events per second (perfect for train telemetry).
- ğŸ”’ Durable & replicated storage â€” messages are persisted and can be replayed for debugging or reprocessing.
- âš–ï¸ Scalable & partitioned â€” process data in parallel across many consumers.
- ğŸ”— Decouples services â€” producers and consumers can evolve independently without tight coupling.
- ğŸ” Replay & event-sourcing friendly â€” rebuild state or add new processors without losing history.
- ğŸ§° Rich ecosystem â€” Kafka Streams, Connect, Schema Registry, and great monitoring tools.
- ğŸŒ Language-agnostic â€” Java, Python and more (we use both in this workshop).
- âœ… Exactly-once support (when configured) â€” accurate aggregates for critical analytics.

> Example (SBB): stream train positions â†’ compute speed averages â†’ power the dashboard & maintenance alerts in real time. ğŸš†ğŸ¯

---

# Topics: Your Data Channels ğŸ“
<!-- _class: compact -->

**Topic** = A category or feed name to which records are published

- Like a table in a database or a folder in a filesystem
- Topics are **multi-subscriber** (many consumers can read the same topic)

### Examples in our workshop:
- `train-locations` - raw position data from trains
- `train-speed-averages` - processed speed analytics

---

# Partitions: Scalability & Parallelism âš¡
<!-- _class: compact -->

- Each topic is divided into **partitions**
- Partitions allow parallel processing
- Messages in a partition are **ordered**
- Each message has an **offset** (sequential ID)
- Our `train-locations` topic has **3 partitions**

```
Topic: train-locations
â”œâ”€â”€ Partition 0: [msg0, msg3, msg6, ...]
â”œâ”€â”€ Partition 1: [msg1, msg4, msg7, ...]
â””â”€â”€ Partition 2: [msg2, msg5, msg8, ...]
```

---

# Producers: Publishing Data ğŸ“¤

Applications that **publish** (write) records to topics

- Can specify which partition to send to (or let Kafka decide)
- Can use keys for consistent routing
- In our workshop: **TrainLocationSimulator** (Java)

### Example:
```
Train T-123 â†’ partition based on train ID â†’ always same partition
```

This ensures all messages from the same train stay in order!

---

# Consumers: Reading Data ğŸ“¥
<!-- _class: compact -->

Applications that **subscribe** to topics and process records

- Maintain an **offset** to track what they've read
- Can rewind and replay messages

### In our workshop:
- **SpeedAnalysisStream** (Kafka Streams)
- **DashboardWebApp** (Java)
- **MaintenanceAlerter** (Python)

---

# Consumer Groups: Team Work! ğŸ‘¥
<!-- _class: compact -->

Multiple consumers working together as a **group**

- Each partition assigned to **only one** consumer in the group
- Enables **parallel processing** and **load balancing**
- If a consumer fails, its partitions are reassigned â†’ **fault tolerance**

### Key Point:
âœ… Same group = share the workload
âœ… Different groups = each gets all messages

---

# Partition Rebalancing âš–ï¸
<!-- _class: compact -->

## What happens when consumers join/leave a group?

**Scenario 1:** 1 consumer, 3 partitions â†’ consumer reads all 3
**Scenario 2:** 3 consumers join â†’ each gets 1 partition
**Scenario 3:** 1 consumer leaves â†’ partitions redistributed

### Benefits:
âœ… Automatic scaling
âœ… Fault tolerance
âœ… Zero message loss
âœ… No duplicate processing

---

# Our Workshop Architecture ğŸ—ï¸
<!-- _class: compact -->

### Services:
1. **Producer:** TrainLocationSimulator (Java) - generates train positions
2. **Stream Processor:** SpeedAnalysisStream (Kafka Streams) - calculates averages
3. **WebSocket Dashboard:** DashboardWebApp (Java) - real-time visualization
4. **Maintenance Alerter:** MaintenanceAlerter (Python) - slow train alerts
5. **Dashboard Consumer:** Demonstrates partition rebalancing

All connected through Kafka topics! ğŸ”„

---

# Kafka Streams ğŸ”„
<!-- _class: compact -->

**Library** for building stream processing applications

- Processes data **as it arrives** (real-time)
- Supports transformations, aggregations, joins, windowing
- No separate cluster needed (unlike Spark/Flink)

### Our Use Case:
1. Read from `train-locations`
2. Calculate average speed per train (10-second window)
3. Write to `train-speed-averages`

---

# Kafka is Language Agnostic ğŸŒ

### Any language can produce/consume from Kafka:
- â˜• Java (most common, best support)
- ğŸ Python
- ğŸŸ¨ JavaScript/Node.js
- ğŸ”µ Go
- ğŸŸ£ C#/.NET
- And many more...

### Workshop Demonstration:
âœ… Java producers and consumers
âœ… Python consumer (maintenance alerter)
âœ… Both read from the **same topic** seamlessly

---

# Kafka Monitoring Tools ğŸ“Š

### Command Line Tools:
```bash
kafka-topics --list
kafka-topics --describe --topic train-locations
kafka-consumer-groups --describe --group dashboard-group
```

### Web Interface:
- **Kafka UI** (localhost:8099)
  - Monitor topics, partitions, consumer groups
  - View messages in real-time
  - Track consumer lag

---

# Where is Kafka Used? ğŸŒ

### Industries & Applications:
- ï¿½ **SBB:** Yes â€” we use Kafka too! (Connected train telemetry and real-time services) ğŸ‰
- ï¿½ğŸ“º **Netflix:** Real-time recommendations & monitoring
- ğŸš— **Uber:** Real-time pricing, trip tracking
- ğŸ’¼ **LinkedIn:** Activity streams, operational metrics
- ğŸ’³ **PayPal:** Risk detection, fraud prevention
- ğŸ¦ **Banking:** Transaction processing, fraud detection
- ğŸ›’ **E-commerce:** Inventory management, order processing

### Common Patterns:
Event sourcing â€¢ CQRS â€¢ Data pipelines â€¢ Microservices communication

---

# Kafka vs RabbitMQ/ActiveMQ ğŸ¥Š

<div class="columns">
<div>

### Kafka Advantages:
âœ… Higher throughput (millions of msgs/sec)
âœ… Persistent storage (replay capability)
âœ… Horizontal scalability
âœ… Built for streaming & big data

</div>
<div>

### Traditional Messaging:
âœ… Complex routing (exchanges, bindings)
âœ… Lower latency for small messages
âœ… Message priority queues
âœ… Simpler request/reply patterns

</div>
</div>

### When to use Kafka:
High volume event streaming â€¢ Log aggregation â€¢ Real-time analytics â€¢ Event sourcing

---

# Delivery Semantics & Guarantees ğŸ“

### Three Delivery Modes:

1. **At-most-once** (0 or 1)
   - Messages may be lost but never redelivered
   - Fastest, lowest guarantee

2. **At-least-once** (1 or more) â­ Default
   - Messages never lost but may be redelivered
   - Most common

3. **Exactly-once** (exactly 1)
   - Most complex, requires idempotent producers
   - Highest guarantee, newer feature

---

# Production Best Practices ğŸ¯

### Design Patterns:
- ğŸ“¦ Use shared libraries for data models (avoid duplication)
- ğŸ—„ï¸ Schema Registry for data contracts (Avro, JSON Schema)
- ğŸ”„ Version your message formats
- ğŸ¯ Use message keys for partitioning logic
- âš¡ Configure appropriate retention policies

### Operational:
- ğŸ“Š Monitor consumer lag
- âš–ï¸ Right-size partition count
- ğŸ”’ Enable authentication & encryption (production)
- ğŸ’¾ Configure replication factor â‰¥ 3
- ğŸ§ª Test rebalancing scenarios

---

# Architectural Simplifications âš ï¸

<div class="columns">
<div>

### Workshop Approach:
- Shared package structure
- `TrainPosition` class copied
- Simple for learning

</div>
<div>

### Production Approach:
- ğŸ“¦ Separate shared library/JAR
- ğŸ—„ï¸ Schema Registry
- ğŸ”„ Proper versioning
- ğŸ—ï¸ Microservice data ownership
- ğŸ“ API contracts

</div>
</div>

**Key Takeaway:** Workshops simplify for clarity; production requires more rigor

---

# Hands-On Time! ğŸ› ï¸

### What You'll Do:
1. âœ… Set up Kafka infrastructure (Docker)
2. âœ… Start the train location producer
3. âœ… Run the stream processor
4. âœ… Launch the WebSocket dashboard
5. âœ… Add Python maintenance alerter
6. âœ… Test partition rebalancing with scaling

### Prerequisites Check:
ğŸ³ Docker & Docker Compose â€¢ â˜• Java JDK 17+ â€¢ ğŸ”¨ Maven â€¢ ğŸ Python 3.8+

---

# Live Demonstration ğŸ¬

### Demo Flow:
1. Show Kafka UI - topics & partitions
2. Start producer - see messages in UI
3. Start stream processor - show transformation
4. Launch dashboard - see real-time updates
5. Start Python alerter - show interoperability
6. Scale consumers - demonstrate rebalancing

**Interactive:** Browser tabs, terminal windows, Kafka UI monitoring

---

# Partition Rebalancing in Action âš–ï¸

### Live Scaling Demo:

```bash
# Start with 1 consumer
docker-compose up -d

# Scale to 3 consumers
docker-compose up -d --scale dashboard-consumer=3

# Scale to 2 consumers
docker-compose up -d --scale dashboard-consumer=2

# Back to 1
docker-compose up -d --scale dashboard-consumer=1
```

**Watch:** Partition reassignment â€¢ Rebalancing events â€¢ Zero message loss

---

# What You Learned Today ğŸ“

1. âœ… **Kafka Fundamentals:** Topics, partitions, producers, consumers
2. âœ… **Consumer Groups:** Workload sharing & fault tolerance
3. âœ… **Partition Rebalancing:** Automatic load balancing
4. âœ… **Language Interoperability:** Java + Python working together
5. âœ… **Stream Processing:** Real-time transformations with Kafka Streams
6. âœ… **Monitoring:** CLI tools & web interfaces
7. âœ… **Scalability:** Horizontal scaling patterns

---

# Avoiding Common Mistakes âš ï¸

### Top Issues:
1. ğŸ”Œ **Connection refused:** Kafka not running â†’ `docker ps`
2. ğŸ”„ **Deserialization errors:** Mismatched serializers
3. ğŸ“ˆ **Consumer lag:** Too few consumers for partition count
4. ğŸ”‘ **Wrong consumer group:** Competing consumers
5. ğŸ• **Offset reset issues:** Check `auto.offset.reset` config

### Debug Commands:
```bash
docker-compose logs kafka
kafka-consumer-groups --describe --group <group-id>
```

---

# Continue Your Kafka Journey ğŸš€

### Deep Dive Topics:
- ğŸ“Š Advanced Kafka Streams (joins, state stores)
- ğŸ”’ Security (SSL, SASL, ACLs)
- ğŸŒ Multi-datacenter replication
- âš¡ Performance tuning
- ğŸ—„ï¸ Schema evolution strategies

### Learning Resources:
- ğŸ“š [Confluent Documentation](https://docs.confluent.io/)
- ğŸ“– "Kafka: The Definitive Guide" (O'Reilly)
- ğŸ“ Confluent Developer courses
- ğŸ™ Apache Kafka official docs

---

# Questions? ğŸ¤”

### Common Questions to Anticipate:
- How does Kafka compare to Kinesis/Pulsar?
- When should I use Kafka vs a traditional database?
- How do I handle schema changes?
- What's the maximum throughput?
- How do I monitor Kafka in production?

**Open forum for your questions!**

---

<!-- _class: lead -->
<!-- _paginate: false -->

# Thank You! ğŸ™

- Workshop repository: [GitHub](https://github.com/Rensykes/kafka-train-workshop)
- Join the community: Kafka Slack/Discord

### Call to Action:
â­ Star the repository
ğŸ”„ Try building your own use case
ğŸ“£ Share your experience

**Happy Streaming!** ğŸš‚ğŸ’¨
